from pdfid import pdfid
import fitz
import os
import argparse
import json
import numpy as np
import pandas as pd
import warnings
import pickle
from joblib import load
# Set warnings to be ignored (filter out all warnings)
warnings.filterwarnings("ignore")
"""
Feature extraction of required columns for PDF malware analysis

"""

# getting data from pdfid


def bytes_to_kb(bytes):
    return bytes / (1024)


info = """

-----------------------  PDF features Extraction    ----------------------
                          (Part of :- SLYFIND)
"""
footer = """
--------------------------------------------------------------------------
-Akshay Nagamalla  
Contact Me :- https://www.linkedin.com/in/akshay-nagamalla/
"""
parser = argparse.ArgumentParser(
    description=info, formatter_class=argparse.RawTextHelpFormatter, epilog=footer)
parser.add_argument('file_path',
                    help='provide path of the String')
parser.add_argument('--output_json_path',
                    help='extract features into JSON format',
                    default="pdf_extracted_features")


args = parser.parse_args()

filename = [args.file_path]  # path of the PDF file

# checking file
if not os.path.exists(filename[0]):
    print("File not found")
    exit()

features = {'FileName': args.file_path}

pdf = fitz.open(filename[0])

try:
    features['Pages'] = pdf.page_count
except:
    features['Pages'] = -1

try:
    features['XrefLength'] = pdf.xref_length()
except:
    features['XrefLength'] = -1

try:
    features['TitleCharacters'] = len(pdf.metadata['title'])
except:
    features['TitleCharacters'] = -1

try:
    features['isEncrypted'] = pdf.is_encrypted
except:
    features['isEncrypted'] = -1

images_count = 0

for i in range(pdf.page_count):
    images_count += len(pdf.get_page_images(0))

emb_count = pdf.embfile_count()
emb_Size_sum = 0
if emb_count != 0:
    try:
        for i in range(pdf.embfile_count()):
            emb_Size_sum = pdf.embfile_info(i)
    except:
        features['EmbeddedFiles'] = -1
    else:
        features['EmbeddedFiles'] = emb_Size_sum/emb_count
else:
    features['EmbeddedFiles'] = 0

try:
    image = 0
    for i in range(pdf.page_count):
        image += pdf.get_page_images(i)
except:
    image = -1
features['Images'] = images_count

try:
    text = 0
    for page in pdf:
        if len(page.get_text().split()):
            text = 1
            break
    features['Text'] = text
except:
    features['Text'] = -1
try:
    options = pdfid.get_fake_options()
    options.scan = True
    options.json = True

    list_of_dict = pdfid.PDFiDMain(filename, options)

    pdf_features = list_of_dict['reports'][0]

    del pdf_features['version']

    # changes column names to corresponding to dataset names
    diff_in_feature_name = {
        'header': 'Header',
        'obj': 'Obj',
        'endobj': 'Endobj',
        'stream': 'Stream',
        'endstream': 'Endstream',
        'xref': 'Xref',
        'trailer': 'Trailer',
        'startxref': 'StartXref',
        '/Page': 'PageNo',
        '/Encrypt': 'Encrypt',
        '/ObjStm': 'ObjStm',
        '/JS': 'JS',
        '/JavaScript': 'Javascript',
        '/AA': 'AA',
        '/OpenAction': 'OpenAction',
        '/AcroForm': 'Acroform',
        '/JBIG2Decode': 'JBIG2Decode',
        '/RichMedia': 'RichMedia',
        '/Launch': 'Launch',
        '/EmbeddedFile': 'EmbeddedFile',
        '/XFA': 'XFA',
        '/Colors > 2^24': 'Colors'
    }
    for curr_name, new_name in diff_in_feature_name.items():
        pdf_features[new_name] = features.pop(curr_name)

    features.update(pdf_features)
except:
    features.update({
        'Header': '-1',
        'Obj': -1,
        'Endobj': -1,
        'Stream': -1,
        'Endstream': -1,
        'Xref': -1,
        'Trailer': -1,
        'StartXref': -1,
        'PageNo': -1,
        'Encrypt': -1,
        'ObjStm': -1,
        'JS': -1,
        'JavaScript': -1,
        'AA': -1,
        'OpenAction': -1,
        'AcroForm': -1,
        'JBIG2Decode': -1,
        'RichMedia': -1,
        'Launch': -1,
        'EmbeddedFile': -1,
        'XFA': -1,
        'Colors': -1})

# try:
#     features['metadata_size'] =  len(str(pdf.metadata).encode('utf-8'))
# except:
#     features['metadata_size'] = -1

# try:
#     uri_count = 0
#     for page in pdf:
#         for link in page.get_links():
#            try:
#                link['uri']
#                uri_count += 1
#            except KeyError:
#                continue

#     features['URI'] = uri_count
# except:
#     features['URI'] = -1

# features['PDF_Size'] = round(bytes_to_kb(os.path.getsize(filename[0])),2)
output_path = args.output_json_path
if output_path != "pdf_extracted_features":
    with open(f'{output_path}.json', 'w') as fp:
        json.dump(features, fp)
else:
    print(features)

with open(f"{output_path}.json", 'r') as file:
    data = json.load(file)
if data["isEncrypted"] == False:
    data["isEncrypted"] = 0
else:
    data["isEncrypted"] = 1
indexx = [1]
x = pd.DataFrame(data, indexx)
x.drop(columns=['FileName'], inplace=True)
# print(x.columns)
# x["Header"] = x["Header"].apply(lambda y: hash(y) % 1000)
# loaded_pipeline = load('src/python/pipeline_with_scaler_and_model.pkl')
# a = x.iloc[0]
# nda = np.array([a])
# print(loaded_pipeline.predict(nda))

model_path = "src/python/pdf_rf_scaled.pkl"
with open(model_path, 'rb') as model_file:
    model = pickle.load(model_file)

encoder_path = "src/python/encoder.pkl"
with open(encoder_path, 'rb') as encoder_file:
    encode = pickle.load(encoder_file)
x["Header"] = encode.transform(x["Header"])
# Load the pickled scaler
# Replace with the actual path to your pickled scaler
scaler_path = 'src/python/scaled.pkl'
with open(scaler_path, 'rb') as scaler_file:
    scaler = pickle.load(scaler_file)

scaled_features = scaler.transform(x.values)
# print(scaled_features)
# Make predictions using the loaded model
prediction = model.predict(scaled_features)
print(prediction)
